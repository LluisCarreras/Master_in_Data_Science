{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programación para *Data Science*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unidad 5: Adquisición de datos en Python - Ejercicios y preguntas\n",
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "\n",
    "Seleccionad y descargad cualquier fichero de datos de alguno de los portales comentados en el Notebook de esta unidad. \n",
    "\n",
    "Cargad los datos en una variable Python (podéis usar [pandas](http://pandas.pydata.org/) si queréis). Mostrad el número de muestras que contiene el conjunto de datos y los atributos disponibles para cada muestra. **(1 punto)**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He triat baixar algun fitxer de dades del repositori de la Universitat de California a Irvine. D'entre tots els que hi havia he baixat el conjunt de dades *Wine Quality*. Aquestes venen en un fitxer *csv*, el fitxer *winequality-red.csv*. L'emmagatzemaré en un *dataframe* de pandas i mostraré les 10 primeres files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "5            7.4              0.66         0.00             1.8      0.075   \n",
      "6            7.9              0.60         0.06             1.6      0.069   \n",
      "7            7.3              0.65         0.00             1.2      0.065   \n",
      "8            7.8              0.58         0.02             2.0      0.073   \n",
      "9            7.5              0.50         0.36             6.1      0.071   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "5                 13.0                  40.0   0.9978  3.51       0.56   \n",
      "6                 15.0                  59.0   0.9964  3.30       0.46   \n",
      "7                 15.0                  21.0   0.9946  3.39       0.47   \n",
      "8                  9.0                  18.0   0.9968  3.36       0.57   \n",
      "9                 17.0                 102.0   0.9978  3.35       0.80   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n",
      "5      9.4        5  \n",
      "6      9.4        5  \n",
      "7     10.0        7  \n",
      "8      9.5        7  \n",
      "9     10.5        5  \n"
     ]
    }
   ],
   "source": [
    "# Respuesta\n",
    "\n",
    "# Importem la llibreria pandas i li assignem un àlies per fer més còmodes les crides.\n",
    "import pandas as pd\n",
    "\n",
    "# Llegeix el fitxer i l'emmagatzema en un dataframe de pandas.\n",
    "data = pd.read_csv('winequality-red.csv', sep=';')\n",
    "\n",
    "# Mostra les 10 primeres files.\n",
    "print data.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 1\n",
    "\n",
    "Queremos saber cuántas personas hay en el espacio en un momento dado. Identificad qué método de la [siguiente API](http://open-notify.org/Open-Notify-API) podemos utilizar para obtener este dato y contestad a las siguientes preguntas. **(1 punto)**\n",
    "\n",
    "1. ¿A qué URL realizaremos la petición?\n",
    "2. ¿Qué tipo de petición HTTP (qué acción) tendremos que realizar a la API para obtener los datos deseados?\n",
    "3. ¿En qué formato obtendremos la respuesta de la API seleccionada?\n",
    "4. ¿En qué campo de la respuesta encontraremos la información que buscamos?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Respuesta\n",
    "\n",
    "La petició la farem a la URL: http://api.open-notify.org/astros.json\n",
    "\n",
    "El tipus de petició serà un GET.\n",
    "\n",
    "La resposta de l'API vindrà donada en format JSON.\n",
    "\n",
    "La informació la trobarem en el camp *number*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "Implementad una función que devuelva el número de personas que hay actualmente en el espacio. **(1.5 puntos)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persones que hi ha actualment a l'espai: 6\n"
     ]
    }
   ],
   "source": [
    "# Respuesta\n",
    "\n",
    "# Importem les llibreries que farem servir.\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Definim una funció que ens retorna el número de persones que hi ha actualment a l'espai.\n",
    "def num_people_in_space():\n",
    "    # Realitzem una petició GET a l'API.\n",
    "    response = requests.get('http://api.open-notify.org/astros.json')\n",
    "    # Transformem el resultat obtingut en un diccionari.\n",
    "    response_dict = json.loads(response.content)\n",
    "    # Retornem el valor del camp number del diccionari.\n",
    "    return response_dict['number']\n",
    "\n",
    "# Fem una crida a la funció i guardem el resultat en una variable.\n",
    "people_in_space = num_people_in_space()\n",
    "# Mostrem el resultat.\n",
    "print \"Persones que hi ha actualment a l'espai: %d\" % people_in_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3\n",
    "\n",
    "Programad una función que muestre la fecha y hora de los diez próximos pases de la estación espacial internacional (ISS) por encima de una localización concreta (especificada por su dirección postal). **(2 puntos)**\n",
    "\n",
    "**Pista**: ¡pensad que podéis combinar resultados de varias API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates i temps de les pròximes 10 vegades que la ISS passarà per Carrer Hannover, 2, Alaior:\n",
      "1\tWed\t29-Nov-2017\t22:10:09\n",
      "2\tWed\t29-Nov-2017\t23:46:47\n",
      "3\tThu\t30-Nov-2017\t01:23:16\n",
      "4\tThu\t30-Nov-2017\t03:02:22\n",
      "5\tThu\t30-Nov-2017\t16:27:13\n",
      "6\tThu\t30-Nov-2017\t18:02:08\n",
      "7\tThu\t30-Nov-2017\t19:39:34\n",
      "8\tThu\t30-Nov-2017\t21:17:32\n",
      "9\tThu\t30-Nov-2017\t22:54:28\n",
      "10\tFri\t-Dec-00:30:50\t1\n"
     ]
    }
   ],
   "source": [
    "# Respuesta\n",
    "\n",
    "# Importem la llibreria googlemaps, que interactuarà amb l'API de google maps.\n",
    "import googlemaps\n",
    "\n",
    "# Importem la llibreria time, que ens ofereix funcions per manejar dates i temps.\n",
    "import time\n",
    "\n",
    "# Importem altres llibreries que farem servir.\n",
    "import json\n",
    "import requests\n",
    "\n",
    "def ISS_passes(address, num_passes=10):\n",
    "\n",
    "    # Asignem a la variable api_key la clau que he obtingut de Google.\n",
    "    api_key = \"AIzaSyAxBfKgd1HpX1to2vUbrj2EWkpY04I-xGw\"\n",
    "\n",
    "    # Inicialitzem el client, indicant la clau d'autenticació.\n",
    "    gmaps = googlemaps.Client(key=api_key)\n",
    "\n",
    "    # Utilitzem l'API de geocodificació per obtenir dades d'una adreça.\n",
    "    geocode_result = gmaps.geocode(address)\n",
    "\n",
    "    # Prenem únicament les coordenades geogràfiques de la'adreça.\n",
    "    coords = geocode_result[0][\"geometry\"][\"location\"]\n",
    "\n",
    "    # Extreiem la latitud i la longitud.\n",
    "    lat = coords['lat']\n",
    "    lng = coords['lng']\n",
    "\n",
    "    # Construim el cos de la petició a l'API.\n",
    "    request = 'http://api.open-notify.org/iss-pass.json?lat=%s&lon=%s&n=%d' % (lat, lng, num_passes)\n",
    "\n",
    "    # Realitzem la petició GET a l'API.\n",
    "    response = requests.get(request)\n",
    "\n",
    "    # Transformem el resultat obtingut en un diccionari.\n",
    "    response_dict = json.loads(response.content)\n",
    "    # Retornem el valor del camp number del diccionari.\n",
    "    passes = response_dict[u'response']   \n",
    "\n",
    "    # Mostrem un missatge de presentació.\n",
    "    print u'Dates i temps de les pròximes %d vegades que la ISS passarà per %s:' % (num_passes, address)\n",
    "\n",
    "    # Iterem sobre la llista.\n",
    "    for idx, pass_time in enumerate(passes):\n",
    "        # Índex de cada iteració.\n",
    "        idx += 1\n",
    "        # Prenem el timestamp de cada passada de la ISS.\n",
    "        rise_time = pass_time[u'risetime']\n",
    "        # Convertim el timestamp en un string amb data i temps.\n",
    "        date_time = time.ctime(rise_time)\n",
    "        # Convertim el string en una llista.\n",
    "        date_time_list = date_time.split(\" \")\n",
    "        # De la llista, prenem el dia de la setmana.\n",
    "        day_of_the_week = date_time_list[0]\n",
    "        # De la llista, prenem el mes.\n",
    "        month = date_time_list[1]\n",
    "        # De la llista, prenem el dia.\n",
    "        day = date_time_list[2]\n",
    "        # De la llista, prenem el temps, en format HH:MM:SS.\n",
    "        hms = date_time_list[3]\n",
    "        # De la llista, prenem l'any.\n",
    "        year = date_time_list[4]\n",
    "        # Mostrem per pantalla la informació de data i temps de cada passada\n",
    "        print \"%d\\t%s\\t%s-%s-%s\\t%s\" % (idx, day_of_the_week, day, month, year, hms)\n",
    "\n",
    "\n",
    "# Aplico la funció amb les dades de la meva adreça.        \n",
    "my_address = 'Carrer Hannover, 2, Alaior'\n",
    "ISS_passes(my_address)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 2\n",
    "\n",
    "¿Qué es la _scrapy shell_? Describid para qué sirve y poned algún ejemplo de su uso. **(1 punto)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta**\n",
    "\n",
    "El *Scrapy shell* és un shell interactiu en el que es pot provar i depurar un codi per fer scrape d'una manera bastant ràpida, sense hever de menester executar l'aranya. Està pensat per ser usat pel testeig de codi d'extracció de dades, però es por utilitzar per testejar qualsevol tipus de codi ja que també és un shell de Python amb totes les seves funcions.\n",
    "\n",
    "El shell s'utilitza per testejar expressions CSS o XPath i veure com es comporten i quines dades extreuen de les pàgines web que s'intenten *\"scrapejar\"*. Permet testejar interactivament les expressions que es desitgin mentre s'està escrivint l'aranya, sense necessitat d'executar l'aranya per testejar cada canvi.\n",
    "\n",
    "El *Scrapy shell* no és més que una consola Python normal (o bé una consola IPython, si està disponible) que proporciona algunes funcions addicionals per conveniència. A més, el *Scrapy shell* automàticament crea objectes convenients a partir de la pàgina baixada, tals com els objectes *Response* i *Selector*, per contingut tant en HTML com en XML.\n",
    "\n",
    "Per exemple, podem veure l'exemple que apareix en el web https://doc.scrapy.org/en/latest/intro/overview.html. A continuació es mostra una aranya que *\"escrapeja\"* cites famoses del web http://quotes.toscrape.com, seguint la paginació:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/tag/humor/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').extract_first(),\n",
    "                'author': quote.xpath('span/small/text()').extract_first(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css('li.next a::attr(\"href\")').extract_first()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara es tractaria de guardar aquest codi en algun fitxer, per exemple *quotes_spider.py* i executar l'aranya emprant la comanda *runspider*:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scrapy runspider quotes_spider.py -o quotes.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quan acaba d'executar-se tindrem al fitxer *quotes.json* una llista de les cites en format JSON, contenint el text i l'autor, amb aquest aspecte:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[{\n",
    "    \"author\": \"Jane Austen\",\n",
    "    \"text\": \"\\u201cThe person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.\\u201d\"\n",
    "},\n",
    "{\n",
    "    \"author\": \"Groucho Marx\",\n",
    "    \"text\": \"\\u201cOutside of a dog, a book is man's best friend. Inside of a dog it's too dark to read.\\u201d\"\n",
    "},\n",
    "{\n",
    "    \"author\": \"Steve Martin\",\n",
    "    \"text\": \"\\u201cA day without sunshine is like, you know, night.\\u201d\"\n",
    "},\n",
    "...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Què és el que ha passat? Quan s'executa la commanda *scrapy runspider quotes_spider.py*, *Scrapy* busca una definició d'una aranya i l'executa mitjançant el seu motor *crawler*. El *crawl* comença fent peticions a les URLs definides a l'atribut *start_urls*, en el nostre cas només una URL, i crida el mètode de la classe, *parse*, passant l'objecte resposta com a argument. En el mètode, s'itera sobre totes les cites emprant un selector CSS, i s'obté un diccionari Python amb l'autor i el text de la cita extrets, busca un link a la següent pàgina i prepara una altra petició emprant el mateix mètode.\n",
    "\n",
    "Aquí es pot veure un dels grans avantatges de *Scrapy*: les peticions són previstes i procesades asíncronament. Això vol dir que *Scrapy * no necessita esperar que una petició finalitzi i estigui procesada, ja que pot enviar una altra petició o fer altres coses al mateix moment. Això també significa que altres peticions poden estar executant-se inclús si alguna petició falla o un error té lloc.\n",
    "\n",
    "A la vegada que això permet fer *crawls* molt ràpids, enviant múltiples peticions concurrents al mateix moment, en un ambient tolerant als falls, *Scrapy* també dona control del *crawl* mitjançant alguns *settings*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 4\n",
    "\n",
    "[KDnuggets](http://www.kdnuggets.com/) es una web con contenido sobre *Data Science*. Entre otros, la web mantiene un listado de ofertas de trabajo relacionadas con el ámbito.  \n",
    "\n",
    "Programad un *crawler* que extraiga los títulos de todas las ofertas de trabajo de KDnuggets. **(2.5 puntos)**\n",
    "\n",
    "Para hacerlo, utilizad la estructura de *crawler* que hemos visto en el Notebook de esta unidad, **modificando únicamente dos líneas de código**:\n",
    "- La URL de inicio.\n",
    "- La expresión XPath que selecciona el contenido que hay que capturar.\n",
    "\n",
    "**Nota**: si la ejecución del *crawler* os devuelve un error `ReactorNotRestartable`, reinciad el *kernel* del Notebook (en el menú, `Kernel` - `Restart`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:scrapy.utils.log:Scrapy 1.4.0 started (bot: scrapybot)\n",
      "INFO:scrapy.utils.log:Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)', 'LOG_ENABLED': False}\n",
      "INFO:scrapy.middleware:Enabled extensions:\n",
      "['scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "INFO:scrapy.middleware:Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "INFO:scrapy.middleware:Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "INFO:scrapy.middleware:Enabled item pipelines:\n",
      "[]\n",
      "INFO:scrapy.core.engine:Spider opened\n",
      "INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6023\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET https://www.kdnuggets.com/jobs/index.html> (referer: None)\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'USF Data Institute: Postdoc'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'The Humanalysts: Data Analytics Team, 9'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Virginia Tech: Assistant Professors in Data Analytics'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'IESEG School of Management: Assistant, Associate or Full Professor in Marketing Analytics'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Apple: Machine Learning-Industrial Methods Engineer'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'American Family Insurance: Director, Data Science & Analytics'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'HelloFresh: Machine Learning Engineer'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'HelloFresh: Big Data Engineer'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'HelloFresh: Senior Data Scientist'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Oak Ridge National Laboratory: Postdoc, Imaging, Signals and Machine Learning'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'University of San Francisco: Assistant Professor, Master of Science in Analytics Program'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Versive: Machine Learning Scientist'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Citrix: Machine Learning / AI Architect \\u2013 Research & Development'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'St. Lawrence University: Data Science \\u2013 Assistant Professor'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Polaris Alpha: Data Scientist'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Walmart: Systems Analyst (Developer, Data Science)'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Walmart: Data Scientist'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Apple: Manager, Data Science \\u2013 Apple Media Products Commerce Engineering'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'James Madison U: Assistant or Associate Professor of Data Science'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Jefferies: Data Scientist'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Monash: Research Fellow \\u2013 Blockchain'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Sainsbury\\u2019s: Sr. Data Scientist'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Sainsbury\\u2019s: Data Scientist'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Utica College: Assistant Professor, Data Scientist/Social Scientist, Tenure Track'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Maryville University: Business Data Analytics Faculty'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Arena: Sr. Data Scientist'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Xavier U. of Louisiana: Assistant Professor, Data Science'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'The Humanalysts: Data Analytics Team, 6'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'RWTH Aachen University: 18 Process Mining Jobs (11 PhDs, 5 PostDocs, and 2 Software Engineers)'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Institute for Defense Analyses: Jr. Data Scientist'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Apple: Manager, Data Science \\u2013 Apple Media Products Commerce Engineering'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Apple: Engineering Project Manager \\u2013 Strategic Data Solutions'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Apple: Data Scientist \\u2013 Apple Media Products, Evaluation and A/B Testing'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'FedEx: HR Data Analytics Advisor'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Retina.AI: Sr. Data Engineer'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Retina.AI: Sr. Data Scientist'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Caserta: Big Data Solutions Architects'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Jimdo: Business Intelligence Analyst'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Jimdo: Data Analyst'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Corios: Consulting Manager'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Monash: Academic Opportunities in Information Technology'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Temple: Faculty Positions (Assistant/Associate/Full Professor)'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Syracuse: Faculty, Data Science'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Arrowstreet Capital: Research Associate, Data Science'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'U. of Illinois at Urbana-Champaign: Open Rank Faculty Positions'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Arterys: Machine Learning Engineer'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Elsevier: Machine Learning Scientist'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Arterys: Machine Learning Scientist'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'HelloFresh: Senior Data Scientist'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'U. of Miami: Assistant Professor, with emphasis on Data Science and Machine Learning'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.kdnuggets.com/jobs/index.html>\n",
      "{'title': u'Hellofresh: Big Data Engineer'}\n",
      "INFO:scrapy.core.engine:Closing spider (finished)\n",
      "INFO:scrapy.statscollectors:Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 247,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 12215,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 11, 29, 21, 42, 51, 306502),\n",
      " 'item_scraped_count': 51,\n",
      " 'log_count/DEBUG': 53,\n",
      " 'log_count/INFO': 7,\n",
      " 'memusage/max': 41496576,\n",
      " 'memusage/startup': 41496576,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2017, 11, 29, 21, 42, 49, 717949)}\n",
      "INFO:scrapy.core.engine:Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Importamos scrapy.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Creamos la araña.\n",
    "class uoc_spider(scrapy.Spider):\n",
    "    \n",
    "    # Asignamos un nombre a la araña.\n",
    "    name = \"uoc_spider\"\n",
    "    \n",
    "    # Indicamos la URL que queremos analizar en primer lugar\n",
    "    # Incluid aquí la URL de inicio:\n",
    "    ################################################\n",
    "    start_urls = [\n",
    "        \"https://www.kdnuggets.com/jobs/index.html\"\n",
    "    ]\n",
    "    ################################################\n",
    "    \n",
    "    # Definimos el analizador.   \n",
    "    def parse(self, response):\n",
    "        # Extraemos el título del grado.   \n",
    "        # Incluid aquí la expresión 'xpath' que nos devuelve los títulos de las ofertas de trabajo.\n",
    "        ################################################  \n",
    "        for t in response.xpath('//ul[@class=\"three_ul\"]/li/b/a/text()'):\n",
    "        ################################################\n",
    "            yield {\n",
    "                'title': t.extract()\n",
    "            }\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Creamos un crawler.\n",
    "    process = CrawlerProcess({\n",
    "        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "        'DOWNLOAD_HANDLERS': {'s3': None},\n",
    "        'LOG_ENABLED': False\n",
    "    })\n",
    "\n",
    "    # Inicializamos el crawler con nuestra araña.\n",
    "    process.crawl(uoc_spider)\n",
    "    \n",
    "    # Lanzamos la araña.\n",
    "    process.start()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 3\n",
    "\n",
    "¿Qué es OAuth? Explicad una situación donde sería útil usar OAuth en vez de un protocolo de autenticación tradicional basado en usuario y contraseña. **(1 punto)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta**\n",
    "\n",
    "OAuth, o *Open Authorization*, és un estàndard obert que permet fluxes simples d'autorització per a llocs web o aplicacions informàtiques. Es tracta d'un protocol que permet realitzar una autorització segura d'una API de mode estàndard i simple per aplicacions d'escritori, telèfons mòbils i pàgines web. Per explicar-ho d'una manera entenedora, podem dir que OAuth permet a un usuari del lloc A compartir la seva informació en el mateix lloc A (proveidor de servei) amb el lloc B (consumidor) sense compartir tota la seva identitat. Per a desenvolupadors de consumidors, OAuth és un mètode per interactuar amb dades protegides i publicar-les. Per desenvolupadors de proveidors de serveis, OAuth proporciona als usuaris un accés a les seves dades a la vegada que protegeix les credencials del seu compte. Aquest mecanisme és utilitzat per companyies com Facebook, Google, Twitter, Github i Microsoft per permetre als usuaris compartir informació sobre els seus comptes amb aplicacions de tercers o llocs web.\n",
    "\n",
    "Per exemple, a Google cada petició a una API ha d'incloure un identificador únic. Aquests identificadors únics permeten a la consola enllaçar peticions a projectes específics. Google suporta dos mecanismes per crear identificadors únics:\n",
    "\n",
    "-- Identificadors de clients OAuth 2.0: Per aplicacions que utilitzen el protocol OAuth 2.0 per fer crides a APIS de Google, es pot utilitzar un identificador ID client de OAuth 2.0 per a generar un token d'accés. El token conté un identificador únic.\n",
    "\n",
    "-- Claus API: Una clau API és un identificador únic que un genera emprant la consola. Utilitzar una clau API no requereix cap mena d'acció ni consentiment per part de l'usuari. Les claus API no garanteixen l'accés a qualsevol informació del compte, i no són utilitzades com a mètode d'autorització.\n",
    "\n",
    "OAuth és un protocol que ofereix a l'usuari una manera d'assegurar que un altre usuari específic té permisos per fer algo. OAuth no està pensat per fer coses com validar la identitat d'un usuari, això ja ho fa un servei d'autenticació. L'autenticació és quan valides la identitat d'un usuari, per exemple demanant-li un *user* i un *password* per loguejar-se, mentres que l'autorització és quan es vol xequejar quins permisos té un usuari ja existent. Això es pot resumir dient que AOuth és un protocol per autorització, no per autenticació. \n",
    "\n",
    "Un exemple molt típic d'una situació en la que és més útil utilitzar OAuth en comptes d'un protocol d'autenticació basat en usuari i contrasenya és el que es veu cada vegada més en voler entrar en certes pàgines en les quals necessitariem loguejar-nos, però en comptes de demanar-nos l'usuari i la contrasenya ens demana si volem rebre l'autorització emprant algun altre proveidor de serveis web, com per exemple Google o Facebook. Així, ens demana d'emprar el fet de ja estar logejats en algun d'aquests proveidors, i si accedim ja rebem l'autorització. Cada vegada és més comú aquest exemple; a mi, particularment, avui mateix m'ha passat, que volguent entrar en el meu compte de Udemy, no he necessitat introduir usuari i contrasenya sinó que ha estat suficient amb acceptar entrar emprant les meves credencials oferides per Google i pel fet d'estar loguejat a Google."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
